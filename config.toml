# SYNTON-DB Default Configuration
#
# 本地开发环境配置文件

[server]
# Host address to bind to (0.0.0.0 for all interfaces)
host = "0.0.0.0"

# gRPC server port
grpc_port = 5571

# REST API server port
rest_port = 5570

# Enable/disable servers
grpc_enabled = true
rest_enabled = true

[storage]
# RocksDB data directory
rocksdb_path = "./data/rocksdb"

# Lance data directory
lance_path = "./data/lance"

# Maximum open files for RocksDB
max_open_files = 5000

# Cache size for RocksDB (in MB)
cache_size_mb = 256

# Enable write-ahead log
wal_enabled = true

[memory]
# Decay scale for the forgetting curve (days)
decay_scale = 20.0

# Retention threshold (0.0-1.0). Nodes below this score are candidates for cleanup
retention_threshold = 0.1

# Initial access score for new nodes (0.0-10.0)
initial_access_score = 5.0

# Access score boost per access
access_boost = 0.5

# Enable periodic decay calculation
periodic_decay_enabled = false

# Interval for decay calculation (in seconds)
decay_interval_secs = 3600

[logging]
# Log level: trace, debug, info, warn, error
level = "info"

# Enable JSON formatted logs
json_format = false

# Enable tracing output
tracing_enabled = true

[graphrag]
# Maximum depth for graph traversal during retrieval
max_traversal_depth = 3

# Maximum nodes to return from hybrid search
max_results = 10

# Weight for vector similarity (0.0-1.0)
vector_weight = 0.7

# Weight for graph proximity (0.0-1.0)
graph_weight = 0.3

# Enable confidence scoring
confidence_scoring = true

[ml]
# Enable ML features
enabled = true

# Backend type: local, openai, ollama
backend = "openai"

# Local model configuration (for local backend)
local_model = "sentence-transformers/all-MiniLM-L6-v2"
device = "cpu"
max_length = 512

# API configuration (for openai/ollama backends)
# For Zhipu AI (GLM): https://open.bigmodel.cn/api/paas/v4
# For OpenAI: https://api.openai.com/v1
# For DeepSeek: https://api.deepseek.com/v1
api_endpoint = "https://open.bigmodel.cn/api/paas/v4"
api_key = "${ZHIPU_API_KEY}"  # Set via environment variable
api_model = "embedding-2"
timeout_secs = 30

# Embedding cache
cache_enabled = true
cache_size = 10000

[instrument]
# Enable instrumentation
enabled = true

# Sampling rate (0.0 to 1.0)
sample_rate = 1.0

# Maximum number of spans to keep in memory
max_spans_in_memory = 10000

# Enable persistence
persistence_enabled = true

# Storage path for traces
trace_path = "./data/traces"

# Format: json, protobuf
format = "json"

# Export configuration
[instrument.export]
# Export endpoint (e.g., OpenTelemetry Collector)
endpoint = "http://localhost:4317"

# Export format: otlp, json
format = "otlp"
